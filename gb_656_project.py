# -*- coding: utf-8 -*-
"""GB_656_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ga5iqYqC7AAaW4MYWadrsmx5W6MLrMMu

# Exploring the data and cleaning the data
"""

import numpy as np 
import matplotlib.pyplot as plt  
import matplotlib.lines as mlines
import pandas as pd   
import seaborn as sns
from random import sample
from sklearn.linear_model import LinearRegression
import scipy.stats as st
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from statsmodels.regression.linear_model import OLS
from statsmodels.tools import add_constant,eval_measures
from sklearn.model_selection import train_test_split
import sys


from sklearn.preprocessing import MinMaxScaler, StandardScaler # For rescaling metrics to fit 0 to 1 range
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error,confusion_matrix, classification_report, roc_curve, auc, ConfusionMatrixDisplay
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.decomposition import PCA
from scipy.spatial.distance import euclidean
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz

"""https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/page/82/
This is the link to the competition we are participating in.
"""

#upload the input data from excel
import io
from google.colab import files
uploaded = files.upload()
file = io.BytesIO(uploaded['training_data.csv'])

df = pd.read_csv('training_data.csv') 
df.head()

df.shape
df['city'].value_counts()

# uploading the Test data (dataset without total_cases)
uploaded = files.upload()
file = io.BytesIO(uploaded['test_data_features.csv'])

test_df = pd.read_csv('test_data_features.csv') 
test_df.head()

test_df.shape

#test data info
test_df.info()

#train data info
df.describe()

df.isnull().sum()

df.drop(['week_start_date'], inplace=True, axis=1)
test_df.drop(['week_start_date'], inplace=True, axis=1)

df.isnull().sum()

df.fillna(method='ffill', inplace=True)
test_df.fillna(method='ffill', inplace=True)
df.isnull().sum()

df.info()

df['city'] = df['city'].astype('str')
test_df['city'] = test_df['city'].astype('str')

#Assigning the sj to 0 and iq to 1 in train dataset
temp = []
for i in range(len(df['city'])):
  if df['city'][i]=='sj':
    temp.append(0)
  else:
    temp.append(1)
df['city'] = temp
#Assigning the sj to 0 and iq to 1 in test dataset
temp = []
for i in range(len(test_df['city'])):
  if test_df['city'][i]=='sj':
    temp.append(0)
  else:
    temp.append(1)
test_df['city'] = temp

df.head()
df['total_cases'].describe()

df.info()

sns.heatmap(df.corr());

plt.scatter(df['weekofyear'],df['total_cases'])
plt.show()

plt.scatter(df['reanalysis_air_temp_k'],df['total_cases'])
plt.show()

plt.scatter(df['reanalysis_min_air_temp_k'],df['total_cases'])
plt.show()

plt.hist(df['total_cases'],bins=100)
plt.xlabel('Total Cases')
plt.show()

n = 0
for i in range(len(df['total_cases'])):
  if(df['total_cases'][i]==0):
    n=n+1
print(n)

sns.pairplot(data=df,
                  y_vars=['total_cases'],
                  x_vars=['year','weekofyear','month','ndvi_ne','ndvi_nw','ndvi_se','ndvi_sw']);

sns.pairplot(data=df,
                  y_vars=['total_cases'],
                  x_vars=['precipitation_amt_mm','reanalysis_air_temp_k','reanalysis_avg_temp_k','reanalysis_dew_point_temp_k','reanalysis_max_air_temp_k']);

sns.pairplot(data=df,
                  y_vars=['total_cases'],
                  x_vars=[
                          'reanalysis_min_air_temp_k','reanalysis_precip_amt_kg_per_m2','reanalysis_relative_humidity_percent'
                          ,'reanalysis_sat_precip_amt_mm','reanalysis_specific_humidity_g_per_kg'  ]);

sns.pairplot(data=df,
                  y_vars=['total_cases'],
                  x_vars=['reanalysis_tdtr_k','station_avg_temp_c',
                          'station_diur_temp_rng_c','station_max_temp_c','station_min_temp_c','station_precip_mm'   ]);

plt.boxplot(df['total_cases']);

"""There are a lot of outliers in the total cases. These are important for the government. But the perfomance metric is given as MAE for the competetion , we have to use that."""

#transform the months
for i in range(len(df['month'])):
  if df['city'][i]==1:
    if df['month'][i]<=4:
      df['weekofyear'][i] = df['weekofyear'][i] + 36
      df['month'][i]=df['month'][i]+8
    else:
      df['weekofyear'][i] = df['weekofyear'][i] - 16
      df['month'][i]=df['month'][i]-4

#transform the months
for i in range(len(test_df['month'])):
  if test_df['city'][i]==1:
    if test_df['month'][i]<=4:
      test_df['weekofyear'][i] = test_df['weekofyear'][i] + 36
      test_df['month'][i]=test_df['month'][i]+8
    else:
      test_df['weekofyear'][i] = test_df['weekofyear'][i] - 16
      test_df['month'][i]=test_df['month'][i]-4

#convert all temperatures to Kelvin
df['station_max_temp_c'] = df['station_max_temp_c']+273.15
df['station_avg_temp_c'] = df['station_avg_temp_c']+273.15
df['station_min_temp_c'] = df['station_min_temp_c']+273.15
df['station_diur_temp_rng_c'] = df['station_diur_temp_rng_c']+273.15
test_df['station_max_temp_c'] = test_df['station_max_temp_c']+273.15
test_df['station_avg_temp_c'] = test_df['station_avg_temp_c']+273.15
test_df['station_min_temp_c'] = test_df['station_min_temp_c']+273.15
test_df['station_diur_temp_rng_c'] = test_df['station_diur_temp_rng_c']+273.15

"""# New features?

"""

df['veg'] = (df['ndvi_ne']+df['ndvi_nw']+df['ndvi_se']+df['ndvi_sw'])/4

test_df['veg'] = (test_df['ndvi_ne']+test_df['ndvi_nw']+test_df['ndvi_se']+test_df['ndvi_sw'])/4

X = df[['city','year','weekofyear','month','ndvi_ne','ndvi_nw','ndvi_se','ndvi_sw','precipitation_amt_mm','reanalysis_air_temp_k',
     'reanalysis_avg_temp_k','reanalysis_dew_point_temp_k','reanalysis_max_air_temp_k',
     'reanalysis_min_air_temp_k','reanalysis_precip_amt_kg_per_m2','reanalysis_relative_humidity_percent'
                          ,'reanalysis_sat_precip_amt_mm','reanalysis_specific_humidity_g_per_kg','reanalysis_tdtr_k','station_avg_temp_c',
                          'station_diur_temp_rng_c','station_max_temp_c','station_min_temp_c','station_precip_mm','veg']]
y = df['total_cases']

scaler = MinMaxScaler()
scaler.fit(X)
X_0 = scaler.transform(X)
X = pd.DataFrame(data = X_0, columns = X.columns)
X.head()

test_df_0 = scaler.transform(test_df)
test_df = pd.DataFrame(data = test_df_0, columns = test_df.columns)
test_df.head()

X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)

"""# Training time

Gamma distribution
"""

model = sm.GLM(y_train, X_train, family=sm.families.Gamma()).fit()

predictions = model.predict(X_test)
predictions = round(predictions,0)
score = eval_measures.meanabs(predictions, y_test)
print(score)

"""Negative Binomial distribution"""

model = sm.GLM(y_train, X_train, family=sm.families.NegativeBinomial()).fit()

predictions = model.predict(X_test)
predictions = round(predictions,0)
score = eval_measures.meanabs(predictions, y_test)
print(score)

"""Random forests"""

rf = RandomForestRegressor(max_features=7, n_estimators=500, random_state=1)
rf.fit(X_train, y_train)

predictions = rf.predict(X_test).astype(int)
score = eval_measures.meanabs(predictions, y_test)
print(score)

Importance_ = pd.DataFrame({'Importance':rf.feature_importances_*100}, index=X_train.columns)
Importance = Importance_.sort_values('Importance', axis=0, ascending=False)[0:20]
Importance.plot(kind='barh', color='b', ).invert_yaxis()
plt.xlabel('Variable Importance')
plt.gca().legend_ = None

results = rf.predict(test_df).astype(int).tolist()

results_df = pd.DataFrame()
results_df['city'] = (pd.read_csv('test_data_features.csv'))['city']
results_df['year'] = test_df['year']
results_df['weekofyear'] = test_df['weekofyear']
results_df['total_cases'] = results

results_df

results_df.to_csv('results.csv', index=False)

rf = RandomForestRegressor(max_features=7,max_depth= 2, n_estimators=500, random_state=1)
rf.fit(X_train, y_train)

Importance_ = pd.DataFrame({'Importance':rf.feature_importances_*100}, index=X_train.columns)
Importance = Importance_.sort_values('Importance', axis=0, ascending=False)[0:20]
Importance.plot(kind='barh', color='b', ).invert_yaxis()
plt.xlabel('Variable Importance')
plt.gca().legend_ = None

boost = GradientBoostingRegressor(n_estimators=5000, learning_rate=0.01,random_state=1)
boost.fit(X_train, y_train)

predictions = boost.predict(X_test).astype(int)
score = eval_measures.meanabs(predictions, y_test)
print(score)

Importance_ = pd.DataFrame({'Importance':boost.feature_importances_*100}, index=X_train.columns)
Importance = Importance_.sort_values('Importance', axis=0, ascending=False)[0:20]
Importance.plot(kind='barh', color='b', ).invert_yaxis()
plt.xlabel('Variable Importance')
plt.gca().legend_ = None

results = boost.predict(test_df).astype(int).tolist()

results_df = pd.DataFrame()
results_df['city'] = (pd.read_csv('test_data_features.csv'))['city']
results_df['year'] = (pd.read_csv('test_data_features.csv'))['year']
results_df['weekofyear'] = (pd.read_csv('test_data_features.csv'))['weekofyear']
results_df['total_cases'] = results

results_df.to_csv('results.csv', index=False)

def backward_selection(X, y, pvalue_threshold = 0.05):
    # start with all the colums
    X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)
    included=list(X.columns)
    score_min = sys.maxsize
    while True:
        print("Features included: ",included)
        model = sm.GLM(y_train, pd.DataFrame(X_train[included]), family=sm.families.NegativeBinomial()).fit()
        predictions = round(model.predict(pd.DataFrame(X_test[included])),0)
        score = eval_measures.meanabs(predictions, y_test)
        print(score)
        pvalues = model.pvalues.iloc[1:] #exclude pvalue of intercept
        max_pvalue = pvalues.max()
        if score_min > score:
            score_min = score
            worst_feature = pvalues.idxmax()
            included.remove(worst_feature)
            print(f"Dropping {worst_feature} with p-value {max_pvalue:.4}")
            continue
        break
    return included

backward_selection(X,y)

"""# Spliting according to cities"""

sj_df = df[df['city']==0]
iq_df = df[df['city']==1]

plt.plot(sj_df['year'],sj_df['total_cases']);

plt.plot(iq_df['year'],iq_df['total_cases']);

sj_X = sj_df[['city','year','month','weekofyear','ndvi_ne','ndvi_nw','ndvi_se','ndvi_sw','precipitation_amt_mm','reanalysis_air_temp_k',
     'reanalysis_avg_temp_k','reanalysis_dew_point_temp_k','reanalysis_max_air_temp_k',
     'reanalysis_min_air_temp_k','reanalysis_precip_amt_kg_per_m2','reanalysis_relative_humidity_percent'
                          ,'reanalysis_sat_precip_amt_mm','reanalysis_specific_humidity_g_per_kg','reanalysis_tdtr_k','station_avg_temp_c',
                          'station_diur_temp_rng_c','station_max_temp_c','station_min_temp_c','station_precip_mm','veg']]
sj_y = sj_df['total_cases']
iq_X = iq_df[['city','year','month','weekofyear','ndvi_ne','ndvi_nw','ndvi_se','ndvi_sw','precipitation_amt_mm','reanalysis_air_temp_k',
     'reanalysis_avg_temp_k','reanalysis_dew_point_temp_k','reanalysis_max_air_temp_k',
     'reanalysis_min_air_temp_k','reanalysis_precip_amt_kg_per_m2','reanalysis_relative_humidity_percent'
                          ,'reanalysis_sat_precip_amt_mm','reanalysis_specific_humidity_g_per_kg','reanalysis_tdtr_k','station_avg_temp_c',
                          'station_diur_temp_rng_c','station_max_temp_c','station_min_temp_c','station_precip_mm','veg']]
iq_y = iq_df['total_cases']

plt.scatter(sj_df['month'],sj_df['total_cases'])
plt.show()

plt.scatter(iq_df['month'],iq_df['total_cases'])
plt.show()

sj_X_train, sj_X_test , sj_y_train, sj_y_test = train_test_split(sj_X, sj_y, test_size=0.20, random_state=1)
iq_X_train, iq_X_test , iq_y_train, iq_y_test = train_test_split(iq_X, iq_y, test_size=0.20, random_state=1)

model = sm.GLM(sj_y_train, sj_X_train, family=sm.families.Gamma()).fit()

predictions = model.predict(sj_X_test)
predictions = round(predictions,0)
score = eval_measures.meanabs(predictions, sj_y_test)
print(score)

model = sm.GLM(iq_y_train, iq_X_train, family=sm.families.Gamma()).fit()

predictions = model.predict(iq_X_test)
predictions = round(predictions,0)
score = eval_measures.meanabs(predictions, iq_y_test)
print(score)

print('MAE for GLM split:' ,(39.468*936+ 10.067*520)/1456)

model = sm.GLM(sj_y_train, sj_X_train, family=sm.families.NegativeBinomial()).fit()

predictions = model.predict(sj_X_test)
predictions = round(predictions,0)
score = eval_measures.meanabs(predictions, sj_y_test)
print(score)

model = sm.GLM(iq_y_train, iq_X_train, family=sm.families.NegativeBinomial()).fit()

predictions = model.predict(iq_X_test)
predictions = round(predictions,0)
score = eval_measures.meanabs(predictions, iq_y_test)
print(score)

print('MAE for GLM negative binomial split:' ,(25.1755*936+ 5.8942*520)/1456)

rf = RandomForestRegressor(max_features=7, n_estimators=500, random_state=1)
rf.fit(sj_X_train, sj_y_train)

predictions = rf.predict(sj_X_test).astype(int)
score = eval_measures.meanabs(predictions, sj_y_test)
print(score)

Importance_ = pd.DataFrame({'Importance':rf.feature_importances_*100}, index=sj_X_train.columns)
Importance = Importance_.sort_values('Importance', axis=0, ascending=False)[0:20]
Importance.plot(kind='barh', color='b', ).invert_yaxis()
plt.xlabel('Variable Importance')
plt.gca().legend_ = None

rf = RandomForestRegressor(max_features=7, n_estimators=500, random_state=1)
rf.fit(iq_X_train, iq_y_train)

predictions = rf.predict(iq_X_test).astype(int)
score = eval_measures.meanabs(predictions, iq_y_test)
print(score)

Importance_ = pd.DataFrame({'Importance':rf.feature_importances_*100}, index=iq_X_train.columns)
Importance = Importance_.sort_values('Importance', axis=0, ascending=False)[0:20]
Importance.plot(kind='barh', color='b', ).invert_yaxis()
plt.xlabel('Variable Importance')
plt.gca().legend_ = None

print('MAE for rf split:' ,(17.2446*936+ 4.8653*520)/1456)

boost = GradientBoostingRegressor(n_estimators=5000, learning_rate=0.01,random_state=1)
boost.fit(sj_X_train, sj_y_train)

predictions = boost.predict(sj_X_test).astype(int)
score = eval_measures.meanabs(predictions, sj_y_test)
print(score)

feature_importance = boost.feature_importances_*100
rel_imp = pd.Series(feature_importance, index=sj_X_train.columns).sort_values(ascending=False, inplace=False)
rel_imp = rel_imp[0:20]
rel_imp.plot(kind='barh', color='b', ).invert_yaxis()
plt.xlabel('Variable Importance');

boost = GradientBoostingRegressor(n_estimators=5000, learning_rate=0.01,random_state=1)
boost.fit(iq_X_train, iq_y_train)

predictions = boost.predict(iq_X_test).astype(int)
score = eval_measures.meanabs(predictions, iq_y_test)
print(score)

feature_importance = boost.feature_importances_*100
rel_imp = pd.Series(feature_importance, index=iq_X_train.columns).sort_values(ascending=False, inplace=False)
rel_imp = rel_imp[0:20]
rel_imp.plot(kind='barh', color='b', ).invert_yaxis()
plt.xlabel('Variable Importance');

print('MAE for GB split:' ,(16.0531*936+ 5.4903*520)/1456)

from io import StringIO  

import pydot
from IPython.display import Image
def print_tree(estimator, features, class_names=None, filled=True):
    tree = estimator
    names = features
    color = filled
    classn = class_names
    
    dot_data = StringIO()
    export_graphviz(estimator, out_file=dot_data, feature_names=features, class_names=classn, filled=filled)
    graph = pydot.graph_from_dot_data(dot_data.getvalue())
    return(graph)

tree_first = DecisionTreeRegressor(max_leaf_nodes=6,criterion="absolute_error")

tree_first.fit(iq_X_train, iq_y_train)
graph, = print_tree(tree_first, features=X_train.columns)
Image(graph.create_png())